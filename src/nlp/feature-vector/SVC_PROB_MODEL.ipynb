{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "global_csv = pd.read_csv('dataset/blog/global.csv', encoding=\"utf-8\").append(pd.read_csv('dataset/news/global.csv', encoding=\"utf-8\")).append(pd.read_csv('dataset/keyword_daum/global.csv', encoding=\"utf-8\"))\n",
    "active_csv = pd.read_csv('dataset/blog/active.csv', encoding=\"utf-8\").append(pd.read_csv('dataset/news/active.csv', encoding=\"utf-8\")).append(pd.read_csv('dataset/keyword_daum/active.csv', encoding=\"utf-8\"))\n",
    "challenge_csv = pd.read_csv('dataset/blog/challenge.csv', encoding=\"utf-8\").append(pd.read_csv('dataset/news/challenge.csv', encoding=\"utf-8\")).append(pd.read_csv('dataset/keyword_daum/challenge.csv', encoding=\"utf-8\"))\n",
    "sincerity_csv = pd.read_csv('dataset/blog/sincerity.csv', encoding=\"utf-8\").append(pd.read_csv('dataset/news/sincerity.csv', encoding=\"utf-8\")).append(pd.read_csv('dataset/keyword_daum/sincerity.csv', encoding=\"utf-8\"))\n",
    "communication_csv = pd.read_csv('dataset/blog/communication.csv', encoding=\"utf-8\").append(pd.read_csv('dataset/news/communication.csv', encoding=\"utf-8\")).append(pd.read_csv('dataset/keyword_daum/communication.csv', encoding=\"utf-8\"))\n",
    "patient_csv = pd.read_csv('dataset/blog/patient.csv', encoding=\"utf-8\").append(pd.read_csv('dataset/news/patient.csv', encoding=\"utf-8\")).append(pd.read_csv('dataset/keyword_daum/patient.csv', encoding=\"utf-8\"))\n",
    "honesty_csv = pd.read_csv('dataset/blog/honesty.csv', encoding=\"utf-8\").append(pd.read_csv('dataset/news/honesty.csv', encoding=\"utf-8\")).append(pd.read_csv('dataset/keyword_daum/honesty.csv', encoding=\"utf-8\"))\n",
    "responsibility_csv = pd.read_csv('dataset/blog/responsibility.csv', encoding=\"utf-8\").append(pd.read_csv('dataset/news/responsibility.csv', encoding=\"utf-8\")).append(pd.read_csv('dataset/keyword_daum/responsibility.csv', encoding=\"utf-8\"))\n",
    "creative_csv = pd.read_csv('dataset/blog/creative.csv', encoding=\"utf-8\").append(pd.read_csv('dataset/news/creative.csv', encoding=\"utf-8\")).append(pd.read_csv('dataset/keyword_daum/creative.csv', encoding=\"utf-8\"))\n",
    "teamwork_csv = pd.read_csv('dataset/blog/teamwork.csv', encoding=\"utf-8\").append(pd.read_csv('dataset/news/teamwork.csv', encoding=\"utf-8\")).append(pd.read_csv('dataset/keyword_daum/teamwork.csv', encoding=\"utf-8\"))\n",
    "\n",
    "def openStopword():\n",
    "    f = open('dataset/stopwords/stopwords.csv', 'r', encoding='utf-8')\n",
    "    reader = csv.reader(f)\n",
    "    stopwords = list()\n",
    "\n",
    "    for row in reader:\n",
    "        stopwords.append(row[0])\n",
    "\n",
    "    return stopwords\n",
    "\n",
    "def tokenizer(raw, pos=[\"Noun\",\"Verb\"], stopword=openStopword()):\n",
    "    return [\n",
    "        word for word, tag in okt.pos(\n",
    "            raw, \n",
    "            norm=True,   # normalize 정제 과정\n",
    "            stem=True    # stemming 정제 과정\n",
    "            )\n",
    "            if len(word) > 1 and tag in pos and word not in stopword\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "okt = Okt()\n",
    "\n",
    "keyword_names = ['글로벌역량', '능동', '도전', '성실', '소통', '인내심', '정직', '주인의식', '창의', '팀워크']\n",
    "all_data = global_csv.append(active_csv).append(challenge_csv).append(sincerity_csv).append(communication_csv)\n",
    "all_data = all_data.append(patient_csv).append(honesty_csv).append(responsibility_csv).append(creative_csv).append(teamwork_csv)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(all_data, test_size=0.33, random_state=42)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorize = TfidfVectorizer(\n",
    "    ngram_range=(1,3), #n-gram 3\n",
    "    tokenizer=tokenizer,\n",
    "    max_df=0.95,\n",
    "    min_df=0,\n",
    "    sublinear_tf=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdm = vectorize.fit_transform(train[\"sentence\"].apply(lambda x: np.str_(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVC_pipeline = Pipeline([\n",
    "                ('tfidf', vectorize),\n",
    "                ('clf', OneVsRestClassifier(LinearSVC(), n_jobs=1)),\n",
    "            ])\n",
    "\n",
    "text_clf_svc = SVC_pipeline.fit(train[\"sentence\"].apply(lambda x: np.str_(x)), train[\"label\"])\n",
    "# train에 있는 sentence와 label로 모델을 만든다. 이것을 predict한 결과를 test의 label로 테스트를 해볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVC_PROB_pipeline = Pipeline([\n",
    "                ('tfidf', vectorize),\n",
    "                ('clf', OneVsRestClassifier(SVC(probability=True), n_jobs=1)),\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/.dlpc/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "text_clf_svc_prob = SVC_PROB_pipeline.fit(train[\"sentence\"].apply(lambda x: np.str_(x)), train[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SVC_PROB_190519.joblib']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import joblib\n",
    "\n",
    "joblib.dump(text_clf_svc_prob, 'SVC_PROB_190519.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 0.9958405244664076\n",
      "test 0.9409866646499786\n"
     ]
    }
   ],
   "source": [
    "print(\"train\",np.mean(text_clf_svc.predict(train[\"sentence\"].apply(lambda x: np.str_(x))) == train[\"label\"]))\n",
    "print(\"test\",np.mean(text_clf_svc.predict(test[\"sentence\"].apply(lambda x: np.str_(x))) == test[\"label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "cj = pd.read_csv(\"dataset/CJ.csv\", encoding=\"utf-8\")\n",
    "sk = pd.read_csv(\"dataset/SK.csv\", encoding=\"utf-8\")\n",
    "lg = pd.read_csv(\"dataset/LG.csv\", encoding=\"utf-8\")\n",
    "hyundai = pd.read_csv(\"dataset/hyundai.csv\", encoding=\"utf-8\")\n",
    "samsung = pd.read_csv(\"dataset/samsung.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "archit = pd.read_csv(\"dataset/architecture.csv\", encoding=\"utf-8\")\n",
    "IT = pd.read_csv(\"dataset/IT.csv\", encoding=\"utf-8\")\n",
    "manage = pd.read_csv(\"dataset/management.csv\", encoding=\"utf-8\")\n",
    "product = pd.read_csv(\"dataset/production.csv\", encoding=\"utf-8\")\n",
    "sales = pd.read_csv(\"dataset/sales.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cj_prob = text_clf_svc_prob.predict_proba(CJ[\"document\"])\n",
    "sk_prob = text_clf_svc_prob.predict_proba(SK[\"document\"])\n",
    "lg_prob = text_clf_svc_prob.predict_proba(LG[\"document\"])\n",
    "hyundia_prob = text_clf_svc_prob.predict_proba(hyundai[\"document\"])\n",
    "samsung_prob = text_clf_svc_prob.predict_proba(samsung[\"document\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archit_prob = text_clf_svc_prob.predict_proba(CJ[\"document\"])\n",
    "IT_prob = text_clf_svc_prob.predict_proba(SK[\"document\"])\n",
    "manage_prob = text_clf_svc_prob.predict_proba(LG[\"document\"])\n",
    "product_prob = text_clf_svc_prob.predict_proba(hyundai[\"document\"])\n",
    "sales_prob = text_clf_svc_prob.predict_proba(samsung[\"document\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(cj_prob))\n",
    "print(type(sk_prob))\n",
    "print(type(lg_prob))\n",
    "print(type(samsung_prob))\n",
    "print(type(hyundai_prob))\n",
    "print(type(archit_prob))\n",
    "print(type(IT_prob))\n",
    "print(type(manage_prob))\n",
    "print(type(product_prob))\n",
    "print(type(sales_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.04467562e-02 7.05937943e-01 1.11781851e-01 2.54034393e-02\n",
      " 8.81388318e-02 4.55867949e-03 1.26688466e-03 3.30487493e-02\n",
      " 9.13387643e-03 2.82988187e-04]\n",
      "[3.52537717e-02 6.50264203e-01 1.54631750e-01 2.02076444e-02\n",
      " 7.10805401e-02 7.31223318e-03 6.45861332e-06 5.79216537e-02\n",
      " 3.29431109e-03 2.74346349e-05]\n",
      "[3.78608867e-02 6.42035316e-01 1.35633849e-01 3.17778888e-02\n",
      " 6.70169453e-02 2.54389122e-02 2.25059004e-05 4.96160935e-02\n",
      " 1.03044619e-02 2.93140179e-04]\n",
      "[3.78867768e-02 6.47128059e-01 1.39605425e-01 3.37944692e-02\n",
      " 9.06473459e-02 1.54920968e-02 9.66145612e-06 3.09573369e-02\n",
      " 3.78152244e-03 6.97307026e-04]\n",
      "[3.36802721e-02 6.20427806e-01 1.70529581e-01 3.92434594e-02\n",
      " 7.35320883e-02 2.21431603e-02 1.36473676e-05 3.33700692e-02\n",
      " 6.55470694e-03 5.05209728e-04]\n"
     ]
    }
   ],
   "source": [
    "print(cj_prob.mean(axis=0))\n",
    "print(sk_prob.mean(axis=0))\n",
    "print(lg_prob.mean(axis=0))\n",
    "print(hyundai_prob.mean(axis=0))\n",
    "print(samsung_prob.mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(archit_prob.mean(axis=0))\n",
    "print(IT_prob.mean(axis=0))\n",
    "print(manage_prob.mean(axis=0))\n",
    "print(product_prob.mean(axis=0))\n",
    "print(sales_prob.mean(axis=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
