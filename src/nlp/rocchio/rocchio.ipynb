{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from IPython.display import display\n",
    "# display(pd.Dataframe(data))\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.font_manager as fm\n",
    "import matplotlib.pyplot as plt\n",
    "from math import pi\n",
    "from pprint import pprint\n",
    "from konlpy.tag import Okt\n",
    "from collections import Counter\n",
    "from sklearn.manifold import TSNE\n",
    "from future.utils import iteritems\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel, cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1783416)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "raw_keyword = ['글로벌역량', '능동', '도전', '성실', '소통', '인내심', '정직', '주인의식', '창의', '팀워크']\n",
    "\n",
    "# tf-idf값 불러오는 과정\n",
    "X = pd.read_csv('dataset/results/large_vector_X.csv', header=None)\n",
    "X = np.array(X)\n",
    "\n",
    "print(X.shape)\n",
    "print(type(X))\n",
    "\n",
    "# 학습 단어(feature) 불러오는 과정\n",
    "f = open('dataset/results/large_vector_X_features.csv', 'r', encoding='utf-8')\n",
    "reader = csv.reader(f)\n",
    "features = list()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in reader:\n",
    "    features.append(i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['가가 가격', '가가 가지다', '가가 걱정', '가가 건립', '가가 계약', '가가 공복', '가가 공사', '가가 관건', '가가 광진구', '가가 그린', '가가 근대', '가가 나오다', '가가 농업', '가가 높아지다', '가가 대폭', '가가 되다', '가가 디자인', '가가 만들다', '가가 무대', '가가 무역', '가가 미국', '가가 미비', '가가 밀라노', '가가 벌이', '가가 보시', '가가 보아', '가가 보유', '가가 부잣집', '가가 분양', '가가 뽑다', '가가 사설', '가가 설비', '가가 소유', '가가 수도권', '가가 스타', '가가 스타일', '가가 시세', '가가 신뢰', '가가 앞서다', '가가 업다', '가가 연극', '가가 영화', '가가 오다', '가가 요동치다', '가가 운동', '가가 위해', '가가 의무', '가가 저지르다', '가가 적용', '가가 제품', '가가 조합', '가가 주식', '가가 직접', '가가 차지', '가가 추가', '가가 토니', '가가 특별', '가가 패피', '가가 평소', '가가 하다', '가가 한미', '가가호호', '가가호호 부모님', '가가호호 소통', '가감', '가감 개진', '가감 경제정책', '가감 고백', '가감 공개', '가감 그대로', '가감 뉴스', '가감 답변', '가감 대화', '가감 드러내다', '가감 라디오스타', '가감 만남', '가감 매력', '가감 반응', '가감 보여주다', '가감 선보이다', '가감 스타', '가감 쓰다', '가감 연지', '가감 오가다', '가감 이슈', '가감 이야기', '가감 이행', '가감 전달', '가감 전해', '가감 조정', '가감 털다', '가감 토크', '가감 판박이', '가감 하다', '가객', '가객 공연', '가객 김윤령', '가객 독창', '가객 추다', '가거도']\n"
     ]
    }
   ],
   "source": [
    "print(features[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# 불용어리스트 불러오는 과정\n",
    "f = open('dataset/stopwords/stopwords.csv', 'r', encoding='utf-8')\n",
    "reader = csv.reader(f)\n",
    "stopwords = list()\n",
    "\n",
    "for row in reader:\n",
    "    stopwords.append(row[0])\n",
    "\n",
    "print(type(features))\n",
    "\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "def read_data(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        data = f.read()\n",
    "        # remove_quotes\n",
    "        data = data.replace(\"‘\", \" \")\n",
    "        data = data.replace(\"’\", \" \")\n",
    "        data = data.replace(\"“\", \" \")\n",
    "        data = data.replace(\"”\", \" \")\n",
    "        data = data.replace(\"`\", \" \")\n",
    "        data = data.replace(\"\\'\", \" \")\n",
    "        data = data.replace(\"\\\"\", \" \")\n",
    "    return data\n",
    "\n",
    "def tokenizer(raw, pos=[\"Noun\",\"Verb\"], stopword=stopwords):\n",
    "    return [\n",
    "        word for word, tag in okt.pos(\n",
    "            raw, \n",
    "            norm=True,   # normalize 정제 과정\n",
    "            stem=True    # stemming 정제 과정\n",
    "            )\n",
    "            if len(word) > 1 and tag in pos and word not in stopword\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기부터 기업별 분석 진행.\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "\n",
    "def read_data(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        json_data = json.load(f)\n",
    "        one_data = [json_data[i] for i in json_data]\n",
    "    return one_data\n",
    "    \n",
    "print('continue...')\n",
    "with open('dataset/resumes/educe_SAMSUNG_2385.json', encoding='utf-8') as json_file:\n",
    "    json_data = json.load(json_file)\n",
    "print('continue......')\n",
    "resume_data = ''\n",
    "\n",
    "for i in json_data:\n",
    "    resume_data += i['document']\n",
    "    \n",
    "print(len(resume_data))\n",
    "\n",
    "srch=[t for t in tokenizer(resume_data) if t in features]\n",
    "\n",
    "print(len(srch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(srch[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 분석할때에는 X대신 X.toarray()를 넣어준다\n",
    "# document term matrix 에서 검색하고자 하는 feature만 뽑아낸다.\n",
    "\n",
    "srch_dtm = np.asarray(X)[:, [\n",
    "    # vectorize.vocabulary_.get 는 특정 feature에 가지고 있는 index값을 리턴한다 (features에 단어와 일치하는 것)\n",
    "    features.index(i) for i in srch\n",
    "]]\n",
    "\n",
    "\n",
    "data = pd.DataFrame(srch_dtm, columns=srch)\n",
    "srch_dtm = data.rename({0:\"글로벌역량\", 1:\"능동\", 2:\"도전\", 3:\"성실\", 4:\"소통\", 5:\"인내심\", 6:\"정직\", 7:\"주인의식\", 8:\"창의\", 9:\"팀워크\"}, axis='index')\n",
    "display(pd.DataFrame(srch_dtm))\n",
    "\n",
    "\n",
    "score = srch_dtm.sum(axis=1)\n",
    "# score는 문장별 feature 합계 점수\n",
    " \n",
    "for i in score.argsort()[::-1]:\n",
    "    if score[i] >= 0:\n",
    "        print('{} // score : {}'.format(raw_keyword[i], score[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"SAMSUNG_2385_dataframe.csv\", mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_score = csv.writer(open(\"SAMSUNG_score.csv\", \"w\", newline=''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_score.writerow([\"score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_score.writerow([score.글로벌역량])\n",
    "csv_file_score.writerow([score.능동])\n",
    "csv_file_score.writerow([score.도전])\n",
    "csv_file_score.writerow([score.성실])\n",
    "csv_file_score.writerow([score.소통])\n",
    "csv_file_score.writerow([score.인내심])\n",
    "csv_file_score.writerow([score.정직])\n",
    "csv_file_score.writerow([score.주인의식])\n",
    "csv_file_score.writerow([score.창의])\n",
    "csv_file_score.writerow([score.팀워크])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/leesunhong/similarity/dataset/input_resume.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-648a137ebc3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# 검색 문장에서 feature를 뽑아냄\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mresume_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/leesunhong/similarity/dataset/input_resume.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-fac7ad335f6f>\u001b[0m in \u001b[0;36mread_data\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mjson_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mone_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mjson_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/leesunhong/similarity/dataset/input_resume.txt'"
     ]
    }
   ],
   "source": [
    "\n",
    "# 검색 문장에서 feature를 뽑아냄\n",
    "resume_data = read_data('/home/leesunhong/similarity/dataset/input_resume.txt')\n",
    "a = tokenizer(resume_data)\n",
    "print(a)\n",
    "\n",
    "srch=[t for t in tokenizer(resume_data) if t in features]\n",
    "\n",
    "print(srch)\n",
    "\n",
    "# 분석할때에는 X대신 X.toarray()를 넣어준다\n",
    "# document term matrix 에서 검색하고자 하는 feature만 뽑아낸다.\n",
    "\n",
    "srch_dtm = np.asarray(X)[:, [\n",
    "    # vectorize.vocabulary_.get 는 특정 feature에 가지고 있는 index값을 리턴한다 (features에 단어와 일치하는 것)\n",
    "    features.index(i) for i in srch\n",
    "]]\n",
    "\n",
    "\n",
    "'''\n",
    "srch_dtm = np.asarray(X.toarray())[:, [\n",
    "    # vectorize.vocabulary_.get 는 특정 feature에 가지고 있는 index값을 리턴한다 (features에 단어와 일치하는 것)\n",
    "    vectorize.vocabulary_.get(i) for i in srch\n",
    "]]\n",
    "'''\n",
    "\n",
    "print(srch_dtm.shape)\n",
    "\n",
    "data = pd.DataFrame(srch_dtm, columns=srch)\n",
    "srch_dtm = data.rename({0:\"글로벌역량\", 1:\"능동\", 2:\"도전\", 3:\"성실\", 4:\"소통\", 5:\"인내심\", 6:\"정직\", 7:\"주인의식\", 8:\"창의\", 9:\"팀워크\"}, axis='index')\n",
    "display(pd.DataFrame(srch_dtm))\n",
    "\n",
    "\n",
    "score = srch_dtm.sum(axis=1)\n",
    "# score는 문장별 feature 합계 점수\n",
    " \n",
    "for i in score.argsort()[::-1]:\n",
    "    if score[i] >= 0:\n",
    "        print('{} // score : {}'.format(raw_keyword[i], score[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
